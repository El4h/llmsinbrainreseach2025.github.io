<!DOCTYPE html>
<html lang='en'>

<head>
    <base href=".">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>LLMs in Brain Research</title>

    <style>
        /* Organizer section styling */
        .organizers-container {
        display: flex;
        justify-content: center;
        gap: 60px;
        margin-top: 20px;
        margin-bottom: 40px;
        }


        .organizer {
        text-align: center;
        }


        .organizer img {
        width: 180px;
        height: 180px;
        object-fit: cover;
        border-radius: 50%;
        box-shadow: 0 4px 10px rgba(0,0,0,0.2);
        transition: transform 0.3s ease;
        }
        .organizer img:hover {
            transform: scale(1.05);
        }


        .organizer-name {
            display: block;
            margin-top: 10px;
            font-size: 1.1em;
            font-weight: bold;
            text-decoration: none;
            color: #004aad;
            transition: color 0.3s ease;
        }


        .organizer-name:hover {
            color: #0077ff;
        }
    </style>
</head>

<body>


    <div class="banner" style="position: relative; max-width: 100%; overflow: hidden;">
        <img src="assets/banner.jpg" alt="Conference Template Banner" style="width: 100%; height: auto;">
        <div class="bottom-right" style="position: absolute; bottom: 20px; right: 20px; font-size: 1.1em; font-weight: 600; color: white; text-align: right; max-width: 250px;">
            27-28 October 2025 <br> Justus Liebig University Giessen
        </div>
    </div>


    <table class="navigation">
        <tr>
            <td class="navigation">
                <a class="current" title="Workshop Home Page" href=".">Home</a>
            <td class="navigation">
                <a title="Workshop Program" href="program">Program</a>
            </td>
            <td class="navigation">
                <a title="Files and Links related to the Workshop" href="resources">Resources</a>
            </td>
            <td class="navigation">
                <a title="Workshop Flyer" href="flyer">Flyer</a>
        </td>
        </tr>
    </table>


    <h2>Program</h2>
    <h3>27 October</h3>


    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                13:00-13:45
            </td>
            <td class="title">
                Improving LLMs as model organisms of human language processing 
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Mariya Toneva
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Large language models (LLMs) align remarkably well with brain activity during reading and listening, yet questions remain about the types of information they capture and their biological relevance. In this talk, I will discuss our recent advances in probing and improving LLMs as model organisms for human language processing. First, I will show that while text-based language models retain predictive power in semantic language regions after removing non-semantic low-level features, speech-based models alarmingly lose nearly all alignment in semantic regions, revealing a fundamental difference in their alignment with brain-relevant semantics.
                Next, I will introduce brain-tuning, a method that fine-tunes language models with brain data recorded while individuals listen to naturalistic speech. Despite using fMRI data that corresponds to less than 1% of the models' pretraining data, brain-tuning 1) improves alignment with semantic brain regions, 2) reduces reliance on low-level features for this alignment, and 3) excitingly, substantially improves performance on semantic downstream tasks. Together, these works strengthen the utility of speech language models as model organisms of language in the brain, and provide new opportunities for cross-pollination between cognitive neuroscience and AI.
                Related work https://aclanthology.org/2024.acl-long.462/ 
                https://openreview.net/pdf?id=KL8Sm4xRn7

            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                13:45-14:30
            </td>
            <td class="title">
                High-Level Visual Representations in the Human Brain Are Aligned With Large Language Models
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Adrien Doerig
            </td>
        </tr>
        <tr>
            <td class="abstract">
                The human brain extracts complex information from visual inputs, including objects, their spatial and semantic interrelations, and their interactions with the environment. However, a quantitative approach to capture this information remains elusive. I will present work where we show that LLM embeddings of scene captions successfully characterise brain activity evoked by viewing the natural scenes. This mapping captures selectivities of different brain areas, and is sufficiently robust that accurate scene captions can be reconstructed from brain activity. Further, we show that neural networks trained to transform image inputs into LLM representations are better aligned with brain representations than a large number of state-of-the-art alternative models, despite being trained on orders-of-magnitude less data. Overall, these results suggest that LLM embeddings of scene captions provide a representational format that accounts for complex information extracted by the brain from visual inputs.
                Related work: https://arxiv.org/abs/2209.11737


            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                14:30-15:00
            </td>
            <td class="title-special">
                Coffee &amp; Tea Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                15:00-17:30
            </td>
            <td class="title">
                Tutorial
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Marianne de Heer Kloots
            </td>
        </tr>
        <tr>
            <td class="abstract">
                You can find the presentation and the notebooks 
                <a href="http://tiny.cc/giessen1" target="_blank">here</a>, 
                <a href="http://tiny.cc/giessen2" target="_blank">here</a>, or 
                <a href="http://tiny.cc/giessen3" target="_blank">here</a>.
            </td>
        </tr>
    </table>
    <h3>28 October</h3>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                10:00-10:45
            </td>
            <td class="title">
                Abstract syntax and fleeting memory in language models and the human mind 
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Micha Heilbron
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Large Language Models (LLMs) excel at predicting brain responses to language, despite not being designed with the brain in mind. This surprising alignment opens two parallel research directions. The first investigates why these brain-agnostic models capture brain responses so well, and what it may teach us about the brain. The second explores how we can actively build cognitive principles into LLMs to test hypotheses and potentially create more brain-like models. In this talk, I present new work showing that (1) high-level syntactic abstractions are a key driver of LLM-brain alignment and (2) human-like fleeting memory improves language learning and brain-alignment in transformer-based language models. These findings demonstrate how cognitive science can both illuminate and improve language models.
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                10:45-11:30
            </td>
            <td class="title">
                Aligning Humans and Language Models: how to investigate emergent properties of geometrical spaces 
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Gemma Roig
            </td>
        </tr>
        <tr>
            <td class="abstract">
                
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                11:30-12:15
            </td>
            <td class="title">
                Circuits, representations, and the brain: Does language processing in LLMs and VLMs reflect human cognitive mechanisms?
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Sandro Pezzelle 
            </td>
        </tr>
        <tr>
            <td class="abstract">
                A central question in current cognitive neuroscience and AI is to what extent large language models (LLMs) and vision-language models (VLMs) resemble the human brain in their mechanisms and representations. In this talk, I present two complementary studies addressing this question from different angles. First, I explore whether LLMs exhibit distinct and localized mechanisms for formal linguistic tasks (e.g., grammar, fluency) and functional tasks (e.g., reasoning, fact retrieval), as observed in the human brain. Using circuit-level analyses, I'll present evidence of separation between these mechanisms, though without the unified formal linguistic network seen in humans. Second, I investigate whether multimodal training improves the alignment of model representations with brain activity. I'll discuss the results of a study using fMRI data, showing that VLMs, particularly transformer-based encoders, achieve higher correlations with activations in language-related areas than language-only models. Together, these findings suggest that using mechanistic tools to dissect model circuits (based on cognitive and neuroscientific evidence) and comparing artificial and human concept representations provide complementary insights into the gap between humans and machines, with the potential of advancing our understanding of language processing in both systems.
            </td>
        </tr>
    </table>
    <table>
        <tr>
            <td class="date" rowspan="2">
                12:15-13:30
            </td>
            <td class="title-special">
                Lunch &amp; Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                13:30-14:15
            </td>
            <td class="title">
                Can Large Language Models (LLMs) generate useful linguistic corpora? Case studies investigating the literacy skills of German children 
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Benjamin Gagl 
            </td>
        </tr>
        <tr>
            <td class="abstract">
                With the development of LLM chat interfaces, the generation of psycholinguistic corpora became feasible. This talk will investigate whether corpora, an essential resource in psycholinguistic research, generated from LLMs are feasible to rely on. Based on initial resource limitations, we started to examine this issue in children's corpora, which are typically smaller. We could derive word frequency estimates from LLM corpora that predicted reading times well, but we also found that the generated texts had poor vocabulary. One could interpret this finding as LLMs have a good representation of child language. We tested this notion by directly comparing LLM and written texts from children. This corpus analytical study showed that LLMs write much more than children, but their vocabulary is still much less rich. Thus, these findings indicate that the representation of child language is relatively poor, while LLMs show promise in replicating certain linguistic features. Still, the LLMs' palette of expression, however, remains underdeveloped. These findings highlight the need to explore LLMs, trigger future developments, and, more importantly, monitor capabilities for recommendations/regulations for usage in vulnerable groups like children.
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                14:15-15:30
            </td>
            <td class="title-special">
                Coffee &amp; Tea Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>

    <table class="plenary">
        <tr>
            <td class="date" rowspan="3">
                15:30-16:15
            </td>
            <td class="title">
                TopoLM: brain-like spatio-functional organization in a topographic language model
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Johannes Mehrer 
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Neurons in the brain are spatially organized such that neighbors on tissue often exhibit similar response profiles. In the human language system, experimental studies have observed clusters for syntactic and semantic categories, but the mechanisms underlying this functional organization remain unclear. Here, building on work from the vision literature, we develop TopoLM, a transformer language model with an explicit two-dimensional spatial representation of model units. By combining a next-token prediction objective with a spatial smoothness loss, representations in this model assemble into clusters that correspond to semantically interpretable groupings of text and closely match the functional organization in the brain's language system. TopoLM successfully predicts the emergence of the spatio-functional organization of a cortical language system as well as the organization of functional clusters selective for fine-grained linguistic features empirically observed in the human cortex. Our results suggest that the functional organization of the human language system is driven by a unified spatial objective, and provide a functionally and spatially aligned model of language processing in the brain. 
                Project site: https://topolm.epfl.ch/ 

            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                16:15-17:30
            </td>
            <td class="title">
                Panel discussion
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Speaker Name (University)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                 <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>

    <footer>
        &nbsp; Design by <a href="https://github.com/mikepierce">Mike Pierce</a>
    </footer>

</body>
</html>

